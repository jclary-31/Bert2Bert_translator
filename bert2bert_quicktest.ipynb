{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57819e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('He was holding in his hand a little wheel of blackened wood; a string of partly burnt squibs was twisted round it; evidently a Catherine wheel from the fireworks display on the fourteenth of July.', 'Young Roy, half choking with laughter, pushes us allfrombehind to hurry us out.', 'Nothing moves yet in that clear wintry lansdcape.', 'I am caught in failure; here is my chance to quicken their curiosity: I decide to explain who this gipsy was, where he came from; his strange fate . . . Boujardon and Delouche do not care to listen.', 'The table has not been laid; we all eat off our knees, each settling where best he can in the dark classroom.', \"They even confided to us, while M. Seurel was starting off again at the head of our party : 'There was another chap as went by. That tall fellow, you know. . .\", 'This, as we knew, would be the only sight the whole day, which would pass like muddy water along the gutter.', 'About midnight I woke up suddenly.', \"And we shall have to find the rest,' he replied, with clenched teeth.\", \"'What's happening?' asked Meaulnes of a peasant boy who was making haste to finish his meal, a felt hat on his head and a table napkin tucked into his waistcoat:\", 'It almost sounded as if two vehicles slowly followed each other at a very slow trot.', 'There was instant uproar loud and clear; glass panes shaken, clogs stamping on stone; one shove bent the iron bar holding the two leaves of the door; but Meaulnes had already turned the little key in the lock, at the risk of cutting himself on its broken ring.', 'Scarcely had he had time even quietly to go over in his mind the beautiful conversation of the morning.', \"Above us, indeed, in a box-room where the blackened remains of the last fourteenth of July fireworks were piled up, an unknown step trod confidently to and fro, shaking the ceiling, crossed the huge dark lumber-rooms of the upper storey and passed at last towards the unused assistant masters' rooms, where lime tree leaves were put to dry and apples to ripen.\", 'It must have been a serious and unexpected threat, for the young fellow could not help turning round and looking at the other, who, to carry it through, grinned and poked his neighbours in the ribs as if to bring them onto his side . . . All this happened in a few seconds.', 'My shoes are red. . .', 'Meaulnes looked between the curtains with caution.', 'Now, on the contrary, it is better to forget me.', 'Was that a last farewell?', \"I recognised Delouche's horse and cart.\", 'But once upstairs in our room, by the light of the lamp which Millie had left us, we both remained a long time mending our overalls and quietly discussing all that had happened, like two brothers in arms on the evening of a lost battle . . .', \"No one will ever prevent Daniel riding on the big bay pony.'\", \"'Not even a rug.\", 'It was a cold Sunday of November, the first day of autumn to make one think of winter.', \"These people must have gone through Father Martin's field and climbed on the low wall separating the field from the playground.\", \"'But suppose Frantz was to come back this very evening with his fiancée?'\", \"We even forgot to tell M. Seurel, as we usually did about four o'clock, the names of the two boys who had to stay to sweep the room.\", 'He showed me two squib lighters with their paper tapers which the flame had singed, before going out.', 'Augustin begged her to stay a moment longer.', 'The race had ended too soon.', 'I am going away.', 'They help me first, with slight condescension, as if I were not one of themselves, not a sportsman, not a peasant. This rather annoys me.', 'They wore pretty hats with strings.', 'He held in his hand a long pole hung with lanterns of many colours, and with his legs crossed he quietly watched his companion work.', 'I waited in vain for a word from Meaulnes all Easter Monday and the following days - days so calm after the Easter fever that just to wait for summer seemed the only thing to be done.', 'Before Meaulnes could say anything, the three of them reached the door of a large room where a big fire was burning. For table, boards had been placed on trestles; white tablecloths had been spread and people of all kinds were dining with ceremony.', \"'What on earth is the matter?'\", 'But this one was not to be forgotten so soon.', \"Look, there is the milestone by the roadside.'\", 'So I was seldom allowed to go out, and I recall that Millie, who was very proud of me, more than once brought me home and boxed my ears for having been caught hopping thus with some village urchins.', 'No one answered.', 'Two women were approaching, one very old and bent, the other a young girl, fair and slender, whose charming dress, after all the fancy costumes of the previous evening, at first appeared strange to Meaulnes.', \"We shall soon know where we've got to.'\", 'The tall white shadow which Meaulnes had seen hurrying amongst the trees, on the last evening of the fête, was Booby, who, having rescued the disconsolate fiancé, was running away with him.', 'And even at that time I still thought that there could be no real holidays in any other place.', 'We used to live with Uncle Florentin and Aunt Julie. They had a boy of my age, Cousin Firmin, and eight daughters, the two eldest of whom, Marie-Louise and Charlotte, might have been seventeen and fifteen.', 'And he entered this passage, glad to have no more hedges and banks to get over.', 'He started back into the alcove and his hobnailed shoes rang against one of the bronze ornaments which he had pushed close to the wall.', \"'Seated on a bench, shivering and wretched, I take pleasure in imagining that some one is going to take me gently by the arm ...\", \"Look at the boarding of this house last night; wasn't it well managed? Didn't it come off well?\", 'She had decided to let Augustin, the elder boy, board with us, that he might take the Higher Course.', 'Besides, night is falling . . . My companions make me go by back ways, then across two gardens, and round a pond, until I find myself back in a muddy wet street in which the lights of the Café Daniel are reflected.', 'Each one picked up his cap or his belt.', 'The horses set off at full gallop and Meaulnes could not make out, at first, whether the peasant who drove was attempting to hold them back or on the contrary urging them on.', 'All day Millie had waited for the station omnibus to bring her a hat for the bad weather.', \"In the square a few villagers had put on their firemen's jackets and piled their arms ; stiff and stamping their feet with cold, they were listening to Boujardon, the corporal, losing himself in theory ...\", 'No use either to ask why he suddenly wanted to go to Paris! . ..', 'With a violent jerk Meaulnes threw him reeling, arms out, to the middle of the room; then gripping Delouche by the neck with one hand and opening the door with the other, he tried to throw him out.', 'Alain-Fournier', 'Then Jasmin went on describing the manor house as if he had spent his life there.', 'For the first time Meaulnes felt the little pang that gets you at the close of too lovely a day.', 'Her furs are cold, her veil damp; she brings in with her a flavour of the outside mist; and as she draws near the fire, I see the flaxen fairness of her hair and the soft outlines of her beautiful face bent towards the flame . . .', 'A man, in a goatskin, holding a lantern, led the first horse of the procession.', 'But even so there is still room for hope . ..', 'THE STRANGE FESTIVAL', 'He had seen nothing on reaching our house.', 'Except for a scar cutting across his temple and disappearing under the mass of hair, it was, just as Admiral Meaulnes had minutely described him to me, the fiancé from the unknown manor.', 'On land everything happened as in a dream.', \"I don't know you.\", 'Then the dull class-work of winter began.', 'We enter our home.', 'I was able to read his name and address on the plate, with my lantern.', 'Father had taken up the lamp and, without waiting, went to open the door which had already been locked.', 'The christening bells stopped suddenly, like festive chimes at a mistaken time and place. Boujardon and his men, rifles slung over their shoulders, dragged off the fire engine at a slow trot, and I saw them disappear at the first turning, followed by four silent urchins, crushing under their heavy boots the twigs on the frozen road, down which I dared not follow them.', \"'You know very well that we are arranging things in our own way.'\", 'He ended it all by emptying the sawdust of her body out of her mouth.', 'They came in sight of the landing-stage.', 'I do not know what will become of me.', 'There was no longer the far horizon, no longer that stretch of pale sky in which sight was lost, but little meadows, still green, with high hedges.', \"His head was by the mare's head and he could feel her warmth and her hard breathing ... He took her to the far end of the meadow and threw the rug over her back; then thrusting aside the branches of the hedge, he again noticed the light which came from an isolated house.\", 'The dandy hurried in; Meaulnes followed him and from his first steps in the corridor, he found himself, without seeing any one, in the midst of laughter, songs, shouts, and chases.', 'In the village street people have begun to form groups.', 'He ran to see them and catch them up, moving noiselessly in his pumps.', 'And in the silence I hear a bird -1 imagine it to be a nightingale, but most likely this is wrong, as nightingales only sing at night - a bird who persists in repeating the same phrase : the voice of the morning, a loving word under the shade of the trees, a charming invitation to a walk amongst the alders.', 'He did not mind going now.', \"'You will go to the station with Mouchebœuf all the same,' says M. Seurel to me.\", 'Meaulnes gently pulled on the reins to go back a few yards and stood up in the cart full of perplexity.', 'VII THE SILK WAISTCOAT', 'Meaulnes dropped his lantern in the snow, calling out:', \"'For a month they've been about in the square waiting for fine weather to give us a play, and must have been hatching some mischief.'\", 'He stopped for an instant in the midst of his troubled walk, leaned over the table, searched in a box, took out several sheets of paper ... By the light of the candle Meaulnes saw in profile very fine and very aquiline features, clean-shaven, under a thick head of hair which was parted on one side.', 'Big Boujardon sits down on the window-sill, and the huge fellow munches Savoy biscuits, facing us with a fat smile.', 'His voice had not been heard, and we had to give up looking for him.', 'He passed close to the fish-pond into which he had looked, that very morning, at his own reflection.', \"'To whom does all this belong?' he asked, pointing to 'all this' with the back of the book folded over his forefinger.\", 'Coming out of the absolute darkness of the alcove, he managed to see fairly well about the room, by the light of the green lantern.', 'Then, taking hold of my hand, he quickly drew me back.', 'As for me, a heavy distress rose in my heart which I had not felt at first.', \"Disregarding this outburst of emotion, the fat man continued to watch the work with his legs crossed, yawned, quietly sniffed, and then turning his back went away with the pole on his shoulder, saying : 'Come on !\", 'Meanwhile, the two women passed close to him, and Meaulnes, motionless, watched the girl.')\n",
      "('He was holding in his hand a little wheel of blackened wood; a string of partly burnt squibs was twisted round it; evidently a Catherine wheel from the fireworks display on the fourteenth of July.',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\juano\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:557: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "c:\\Users\\juano\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:577: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/10 done\n",
      "Loss: 12.095444679260254\n",
      " Generated words: ['- - - 提 携 携 携 攜 携 攜 攜 攜 携 便 携 携 帶 带 帶 带 带 带 帶 帶 帶 攜 携 带 带 攜 攜 带 帶 攜 帶 帶 带 攜 帶 带 帯 带 带 携 携 带 攜 带 带 帶 帶 携 帶 帶 帶 带 带 帶 带 带']\n",
      "epoch 2/10 done\n",
      "Loss: 6.216301918029785\n",
      " Generated words: ['visaugi mature mature matureessen mature mature adulte mature matureicata mature mature ave mature mature Astragalus mature mature صاحب mature mature strict mature mature పౌర mature mature gaya mature mature깃 mature mature style mature mature 펙 mature maturesite mature maturegant mature mature vite mature maturesica mature maturerain mature maturepara mature mature conforme mature mature삿 mature']\n",
      "epoch 3/10 done\n",
      "Loss: 3.8064959049224854\n",
      " Generated words: [\",..,, à à à et à à, à de à à une à, d à à par à à tout, à d d à au à - à à se à à termine tout à à pour à à à à chi à à d, à ' à à au au à à les à\"]\n",
      "epoch 4/10 done\n",
      "Loss: 2.886213779449463\n",
      " Generated words: [',,...,, à à,., de,,,,, la,, d.,., à une, à.,. à à. à, à à et., une., un, à,, pour,,s à, à et,']\n",
      "epoch 5/10 done\n",
      "Loss: 2.653719425201416\n",
      " Generated words: [\"##чайчай...,. - -,... - : de de et une la les 's un à pour sur que\"]\n",
      "epoch 6/10 done\n",
      "Loss: 2.488603353500366\n",
      " Generated words: [', de un une et,. au às é pour la tout qui avait avec il dans. une une pour,']\n",
      "epoch 7/10 done\n",
      "Loss: 2.310683250427246\n",
      " Generated words: ['Mais, d il à,. de le é la un qui au l pour et une m app dans en']\n",
      "epoch 8/10 done\n",
      "Loss: 2.140576124191284\n",
      " Generated words: ['nous de la une qui un un, il que pour l à les en m et dans le où au']\n",
      "epoch 9/10 done\n",
      "Loss: 1.9941940307617188\n",
      " Generated words: ['il avait avait un eut de dans le et une pour à é, la l en comme même avec deux']\n",
      "epoch 10/10 done\n",
      "Loss: 1.8887219429016113\n",
      " Generated words: ['il avait qui pour comme et une petite petite petite porte porte porte de porte porte, à la porte porte d un petit petit petit de petit petit grand petit grand grand grand petit petit ou ou dans']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    BertTokenizerFast, BertConfig, BertLMHeadModel, BertModel,\n",
    "    AutoModel, EncoderDecoderModel, AutoTokenizer,\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# pip install -U transformers datasets\n",
    "import random, math\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, BertConfig, BertLMHeadModel, EncoderDecoderModel\n",
    ")\n",
    "\n",
    "\n",
    "####\n",
    "SRC_CKPT = \"bert-base-uncased\"              # encoder (EN)\n",
    "TGT_CKPT = \"bert-base-multilingual-cased\"   # decoder (FR-capable)\n",
    "\n",
    "\n",
    "#########the data\n",
    "ds = load_dataset(\"Helsinki-NLP/opus_books\", \"en-fr\", split=\"train\")  # ~1M pairs\n",
    "\n",
    "pairs = [(ex[\"translation\"][\"en\"], ex[\"translation\"][\"fr\"]) for ex in ds.select(range(2000))]\n",
    "random.shuffle(pairs)\n",
    "pairs = pairs[:100]  # exactly 100\n",
    "\n",
    "src_list, tgt_list = zip(*pairs)\n",
    "#print(src_list)\n",
    "\n",
    "\n",
    "# ---- tokenizers\n",
    "torch.manual_seed(0)\n",
    "enc =  \"bert-base-uncased\"\n",
    "dec= \"bert-base-multilingual-cased\"   # decoder (FR-capable)\n",
    "\n",
    "tok_src = BertTokenizerFast.from_pretrained(enc)\n",
    "tok_tgt = BertTokenizerFast.from_pretrained(dec)\n",
    "PAD_ID = tok_tgt.pad_token_id\n",
    "EOS_ID = tok_tgt.sep_token_id\n",
    "BOS_ID = tok_tgt.cls_token_id\n",
    "\n",
    "MAX_SRC_LEN=100\n",
    "\n",
    "def tokenize_src(text):\n",
    "    x=tok_src(text,\n",
    "              padding='max_length', \n",
    "              truncation=True, \n",
    "              max_length=MAX_SRC_LEN, \n",
    "              return_tensors=\"pt\"\n",
    "              )\n",
    "    return x    \n",
    "\n",
    "def tokenize_tgt(text):\n",
    "    x=tok_tgt(text,\n",
    "              padding='max_length',\n",
    "              truncation=True, \n",
    "              max_length=MAX_SRC_LEN, \n",
    "              return_tensors=\"pt\"\n",
    "              )     \n",
    "    return x\n",
    "\n",
    "\n",
    "X=tokenize_src(src_list)\n",
    "Y=tokenize_tgt(tgt_list)\n",
    "\n",
    "\n",
    "print(src_list[:1])\n",
    "\n",
    "BATCH_SIZE=8\n",
    "\n",
    "train_dl = DataLoader(TensorDataset( X['input_ids'],X['attention_mask'],Y['input_ids']),\n",
    "                                     batch_size=BATCH_SIZE,\n",
    "                                     shuffle=True)\n",
    "\n",
    "\n",
    "# X2 = tok_src([\"cats are cute\", \"i like tea\"],\n",
    "#               return_tensors=\"pt\",\n",
    "#               padding=True,\n",
    "#               truncation=True)\n",
    "\n",
    "\n",
    "# Y2 = tok_tgt([\"les chats sont mignons\", \"j'aime le thé\"], \n",
    "#              return_tensors=\"pt\",\n",
    "#              padding=True,\n",
    "#              truncation=True,\n",
    "#              add_special_tokens=False)  # no [CLS]\n",
    "\n",
    "\n",
    "# labels2 = Y2.input_ids#.clone() \n",
    "# labels2[labels2 == tok_tgt.pad_token_id] = -100\n",
    "\n",
    "\n",
    "\n",
    "######### model!\n",
    "\n",
    "\n",
    "\n",
    "# ---------- CORRECT: set decoder_start_token_id ON CONFIG before forward\n",
    "dec_cfg_ok = BertConfig.from_pretrained(dec,\n",
    "                                         is_decoder=True,\n",
    "                                         add_cross_attention=True)\n",
    "\n",
    "\n",
    "good = EncoderDecoderModel(encoder=AutoModel.from_pretrained(enc),\n",
    "                           decoder=BertLMHeadModel.from_pretrained(dec, config=dec_cfg_ok),\n",
    "                           )\n",
    "\n",
    "# Required for loss computation (right-shift uses this)\n",
    "good.config.decoder_start_token_id = tok_tgt.cls_token_id\n",
    "good.config.eos_token_id = tok_tgt.sep_token_id\n",
    "good.config.pad_token_id = tok_tgt.pad_token_id\n",
    "good.config.vocab_size = good.config.decoder.vocab_size\n",
    "good.config.tie_encoder_decoder = True\n",
    "\n",
    "#add optimizer\n",
    "optimizer = AdamW(good.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "model=good\n",
    "# train then test again\n",
    "model.train()\n",
    "opt = AdamW(good.parameters(), lr=1e-5)\n",
    "\n",
    "steps = 0\n",
    "EPOCHS=10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    Loss=0\n",
    "    for ids,mask, labels in train_dl:\n",
    "        opt.zero_grad()\n",
    "        out = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "        out.loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        steps += 1\n",
    "        Loss+=out.loss\n",
    "    print(f\"epoch {epoch+1}/{EPOCHS} done\")\n",
    "    print(f'Loss: {Loss/len(train_dl)}')\n",
    "\n",
    "\n",
    "    \n",
    "    X=tokenize_src(src_list[:1])\n",
    "    gen=model.generate( X['input_ids'],attention_mask= X['attention_mask'],\n",
    "                        num_beams=4, max_new_tokens=64, early_stopping=True,\n",
    "                        decoder_start_token_id=BOS_ID, eos_token_id=EOS_ID, pad_token_id=PAD_ID,\n",
    "                        bad_words_ids=[[PAD_ID]],          # block PAD\n",
    "                        repetition_penalty=1.1,            # mild\n",
    "                        no_repeat_ngram_size=3             # optional hygiene \n",
    "                        )   \n",
    "#     print('run', str(i))\n",
    "#     print('loss', loss.detach().numpy())\n",
    "#     #print('Generated token:',gen2)\n",
    "    print(\" Generated words:\", [tok_tgt.decode(g, skip_special_tokens=True) for g in gen])\n",
    "#     print('\\n')    \n",
    "#model.eval()\n",
    "\n",
    "\n",
    "# good.train()\n",
    "# for i in range(0,10):\n",
    "#     out = good(input_ids=X2[\"input_ids\"],\n",
    "#             attention_mask=X2[\"attention_mask\"],\n",
    "#             labels=labels2) \n",
    "\n",
    "#     loss=out.loss\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     gen2 = good.generate(X2[\"input_ids\"],\n",
    "#                         attention_mask=X2[\"attention_mask\"],\n",
    "#                         num_beams=4,\n",
    "#                         #max_new_tokens=24,\n",
    "#                         no_repeat_ngram_size=3,\n",
    "#                         #early_stopping=True,\n",
    "#                         decoder_start_token_id=tok_tgt.cls_token_id,\n",
    "#                         # eos_token_id=tok_tgt.sep_token_id,\n",
    "#                         # pad_token_id=tok_tgt.pad_token_id\n",
    "#                         )\n",
    "\n",
    "#     print('run', str(i))\n",
    "#     print('loss', loss.detach().numpy())\n",
    "#     #print('Generated token:',gen2)\n",
    "#     print(\" Generated words:\", [tok_tgt.decode(g, skip_special_tokens=True) for g in gen2])\n",
    "#     print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
