{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57819e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 0\n",
      "loss 11.266043\n",
      " Generated words: ['the i school school school a school school me school school up school school kindergarten school school two school school', 'i i i ce ce ce ke ce ce se ce ce ko ce ce ka ce cece ce']\n",
      "\n",
      "\n",
      "run 1\n",
      "loss 9.112206\n",
      " Generated words: ['. it it it him him him me me me him me him him sen sen sen me me sen', 'the. school school school sam sam sam school school so so so sam sam so sam school sam school']\n",
      "\n",
      "\n",
      "run 2\n",
      "loss 9.4951725\n",
      " Generated words: ['is is is can can can is is while while while can can while while like like like while while', 'a is is is be be be is is so so so is is it so so as as as']\n",
      "\n",
      "\n",
      "run 3\n",
      "loss 8.448451\n",
      " Generated words: ['##sss is is isssnessnessnessss sirss sen sen sens', 'know know know is is is know know be be be is is be be so so so is is']\n",
      "\n",
      "\n",
      "run 4\n",
      "loss 7.659356\n",
      " Generated words: ['##sss type type typessnessnessnessss sulnessness sulnesssness', \"##sss issstssshsss 'ss sirssess\"]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    BertTokenizerFast, BertConfig, BertLMHeadModel, BertModel,\n",
    "    AutoModel, EncoderDecoderModel, AutoTokenizer,\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "enc = dec = \"bert-base-uncased\"\n",
    "tok_src = BertTokenizerFast.from_pretrained(enc)\n",
    "tok_tgt = tok_src#BertTokenizerFast.from_pretrained(dec)\n",
    "\n",
    "\n",
    "# ---------- CORRECT: set decoder_start_token_id ON CONFIG before forward\n",
    "dec_cfg_ok = BertConfig.from_pretrained(dec,\n",
    "                                         is_decoder=True,\n",
    "                                         add_cross_attention=True)\n",
    "\n",
    "\n",
    "good = EncoderDecoderModel(encoder=AutoModel.from_pretrained(enc),\n",
    "                           decoder=BertLMHeadModel.from_pretrained(dec, config=dec_cfg_ok),\n",
    "                           )\n",
    "\n",
    "# Required for loss computation (right-shift uses this)\n",
    "good.config.decoder_start_token_id = tok_tgt.cls_token_id\n",
    "good.config.eos_token_id = tok_tgt.sep_token_id\n",
    "good.config.pad_token_id = tok_tgt.pad_token_id\n",
    "good.config.vocab_size = good.config.decoder.vocab_size\n",
    "good.config.tie_encoder_decoder = True\n",
    "\n",
    "\n",
    "#add optimizer\n",
    "optimizer = AdamW(good.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "X2 = tok_src([\"cats are cute\", \"i like tea\"],\n",
    "              return_tensors=\"pt\",\n",
    "              padding=True,\n",
    "              truncation=True)\n",
    "\n",
    "\n",
    "Y2 = tok_tgt([\"les chats sont mignons\", \"j'aime le thé\"], \n",
    "             return_tensors=\"pt\",\n",
    "             padding=True,\n",
    "             truncation=True,\n",
    "             add_special_tokens=False)  # no [CLS]\n",
    "\n",
    "\n",
    "labels2 = Y2.input_ids#.clone() \n",
    "labels2[labels2 == tok_tgt.pad_token_id] = -100\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0,5):\n",
    "    out = good(input_ids=X2[\"input_ids\"],\n",
    "            attention_mask=X2[\"attention_mask\"],\n",
    "            labels=labels2) \n",
    "\n",
    "    loss=out.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    gen2 = good.generate(X2[\"input_ids\"],\n",
    "                        attention_mask=X2[\"attention_mask\"],\n",
    "                        #num_beams=4,\n",
    "                        #max_new_tokens=24,\n",
    "                        no_repeat_ngram_size=3,\n",
    "                        #early_stopping=True,\n",
    "                        decoder_start_token_id=tok_tgt.cls_token_id,\n",
    "                        # eos_token_id=tok_tgt.sep_token_id,\n",
    "                        # pad_token_id=tok_tgt.pad_token_id\n",
    "                        )\n",
    "\n",
    "    print('run', str(i))\n",
    "    print('loss', loss.detach().numpy())\n",
    "    #print('Generated token:',gen2)\n",
    "    print(\" Generated words:\", [tok_tgt.decode(g, skip_special_tokens=True) for g in gen2])\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
