{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873fd42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/12 done ; loss is 7.759047508239746\n",
      "epoch 2/12 done ; loss is 6.004733562469482\n",
      "epoch 3/12 done ; loss is 4.981064319610596\n",
      "epoch 4/12 done ; loss is 4.067440509796143\n",
      "epoch 5/12 done ; loss is 3.366668224334717\n",
      "epoch 6/12 done ; loss is 2.7335917949676514\n",
      "epoch 7/12 done ; loss is 2.2331979274749756\n",
      "epoch 8/12 done ; loss is 1.7983659505844116\n",
      "epoch 9/12 done ; loss is 1.3819419145584106\n",
      "epoch 10/12 done ; loss is 1.1016132831573486\n",
      "epoch 11/12 done ; loss is 0.9002933502197266\n",
      "epoch 12/12 done ; loss is 0.7752261757850647\n",
      "\n",
      "--- AFTER ---\n",
      "EN: As for me, I found myself obliged, the first time for months, to face alone a long Thursday evening - with the clear feeling that the old carriage had borne away my youth forever.\n",
      "FR_gold: Quant à moi, je me trouvai, pour la première fois depuis de longs mois, seul en face d’une longue soirée de jeudi – avec l’impression que, dans cette vieille voiture, mon adolescence venait de s’en aller pour toujours.\n",
      "FR_pred: Quant à moi, je me trouvai, pour la première fois depuis depuis de longs mois, seul en face d une longue soirée de jeudi avec l impression que, dans cette voiture, mon adolescence venait de s en aller pour toujours.\n",
      "-\n",
      "EN: No one asked him who Booby was.\n",
      "FR_gold: Personne ne lui demanda qui était Ganache.\n",
      "FR_pred: Personne ne lui demanda qui était Ganache.\n",
      "-\n",
      "EN: M. Seurel's here .. .'\n",
      "FR_gold: M. Seurel est là…\n",
      "FR_pred: M. Seurel est M..\n",
      "-\n",
      "EN: After the ball where everything was charming but feverish and mad, where he had himself so madly chased the tall Pierrot, Meaulnes found that he had dropped into the most peaceful happiness on earth.\n",
      "FR_gold: Après cette fête où tout était charmant, mais fiévreux et fou, où lui-même avait si follement poursuivi le grand pierrot, Meaulnes se trouvait là plongé dans le bonheur le plus calme du monde.\n",
      "FR_pred: Après cette fête où tout était charmant, mais fiévreux et fou, où lui - même avait si follement poursuivi le grand pierrot, Meaulnes se trouvait là - bas.\n",
      "-\n",
      "EN: At half-past eight, just as M. Seurel was giving the signal to enter school, we arrived, quite out of breath, to line up.\n",
      "FR_gold: À huit heures et demie, à l’instant où M. Seurel allait donner le signal d’entrer, nous arrivâmes tout essoufflés pour nous mettre sur les rangs.\n",
      "FR_pred: À huit heures et demie, à l instant où M. Seurel allait donner le signal d entrer, nous arrivâmes tout essoufflés pour nous mettre sur les rangs.\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "# pip install -U transformers datasets\n",
    "#import os\n",
    "#os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "import random, math\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, BertConfig, BertLMHeadModel, EncoderDecoderModel\n",
    ")\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# ---- config\n",
    "SEED = 0\n",
    "SRC_CKPT = \"bert-base-uncased\"              # encoder (EN)\n",
    "TGT_CKPT = \"bert-base-multilingual-cased\"   # decoder (FR-capable)\n",
    "MAX_SRC_LEN = 128\n",
    "MAX_TGT_LEN = 128\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 12                                 # raise to 20–30 if not overfitting\n",
    "LR = 5e-5\n",
    "traindl_opt=1\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---- tokenizers\n",
    "tok_src = BertTokenizer.from_pretrained(SRC_CKPT)\n",
    "tok_tgt = BertTokenizer.from_pretrained(TGT_CKPT)\n",
    "PAD_ID = tok_tgt.pad_token_id\n",
    "EOS_ID = tok_tgt.sep_token_id\n",
    "BOS_ID = tok_tgt.cls_token_id\n",
    "\n",
    "# ---- model: BERT encoder + BERT LM-head decoder with cross-attn\n",
    "\n",
    "#ref\n",
    "# dec_cfg = BertConfig.from_pretrained(TGT_CKPT, is_decoder=True, add_cross_attention=True)\n",
    "# encoder=AutoModel.from_pretrained(SRC_CKPT),\n",
    "# decoder=BertLMHeadModel.from_pretrained(TGT_CKPT, config=dec_cfg),\n",
    "\n",
    "encoder = AutoModel.from_pretrained(SRC_CKPT,\n",
    "                                    bos_token_id=BOS_ID,\n",
    "                                    eos_token_id=EOS_ID\n",
    "                                    )\n",
    "\n",
    "decoder=BertLMHeadModel.from_pretrained(TGT_CKPT,\n",
    "                                        add_cross_attention=True,\n",
    "                                        is_decoder=True,\n",
    "                                        bos_token_id=BOS_ID,\n",
    "                                        eos_token_id=EOS_ID\n",
    "                                        )\n",
    "\n",
    "model = EncoderDecoderModel(encoder=encoder,\n",
    "                           decoder=decoder\n",
    "                           ).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# required special ids for training (right-shift) and decode\n",
    "model.config.decoder_start_token_id = BOS_ID\n",
    "model.config.eos_token_id = EOS_ID\n",
    "model.config.pad_token_id = PAD_ID\n",
    "model.config.tie_encoder_decoder = False\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# ---- tiny EN–FR set: take 100 pairs from OPUS Books\n",
    "# notes: you can replace this with your own parallel lists\n",
    "ds = load_dataset(\"Helsinki-NLP/opus_books\", \"en-fr\", split=\"train\")  # ~1M pairs\n",
    "pairs = [(ex[\"translation\"][\"en\"], ex[\"translation\"][\"fr\"]) for ex in ds.select(range(2000))]\n",
    "random.shuffle(pairs)\n",
    "pairs = pairs[:100]  # exactly 100\n",
    "src_list, tgt_list = zip(*pairs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####\n",
    "def tokenize_src(Batchoftext):\n",
    "    ''' \n",
    "    transform a batch of sentences into token with id and attention mask with help of Bert pretrainmodel\n",
    "    Bert pretrained model \n",
    "    Some text are too long... i think this will cause error, see later\n",
    "    '''\n",
    "    # see https://medium.com/axinc-ai/how-tokenizer-parameters-impact-transformers-behavior-8be8030637c6\n",
    "    Tok = tok_src(Batchoftext,\n",
    "                    max_length=MAX_SRC_LEN,\n",
    "                    #padding='max_length',\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    return_tensors='pt'# pytorch tensor\n",
    "                    )\n",
    "\n",
    "    return Tok#Tok['input_ids'], Tok['attention_mask']\n",
    "\n",
    "def tokenize_tgt(Batchoftext):\n",
    "    ''' \n",
    "    same as source, but for targeted language,\n",
    "    + BOS is not needed for decoder\n",
    "    + EOS absent in cutted sentence\n",
    "    '''\n",
    "    Tok = tok_tgt(Batchoftext,\n",
    "                    max_length=MAX_SRC_LEN,\n",
    "                    padding='max_length',\n",
    "                    truncation=True,\n",
    "                    add_special_tokens=False,\n",
    "                    return_tensors='pt'# pytorch tensor\n",
    "                    )['input_ids']\n",
    "    \n",
    "    Tok_fixed = torch.full_like(Tok, PAD_ID)\n",
    "    for i in range(Tok.shape[0]):\n",
    "        y=Tok[i,:]\n",
    "        y=y[y!=PAD_ID] #remove all padd\n",
    "        if len(y)<MAX_SRC_LEN: #add EOS if max length not reached\n",
    "            y=torch.cat((y,torch.tensor([EOS_ID])),dim=0)\n",
    "        Tok_fixed[i,:len(y)]=y\n",
    "\n",
    "    Tok_fixed[Tok_fixed == PAD_ID] = -100\n",
    "    \n",
    "    return Tok_fixed\n",
    "\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,src_list,tgt_list):\n",
    "        self.src=src_list\n",
    "        self.tgt=tgt_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.src[index], self.tgt[index]    \n",
    "        \n",
    "\n",
    "def collate(batch):\n",
    "    s, t = zip(*batch)\n",
    "    z=tokenize_src(s)\n",
    "    train_input_ids=z['input_ids']\n",
    "    train_attention_masks=z['attention_mask']\n",
    "    train_labels_ids=tokenize_tgt(t) \n",
    "\n",
    "    return train_input_ids, train_attention_masks, train_labels_ids\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### this two approach should be the same, no ? but first one give errors \n",
    "\n",
    "def compute_data_dl(src_list,tgt_list,option=None):\n",
    "    ''' two method for computing train dataloader, which is used for training'''\n",
    "\n",
    "    if option==None:\n",
    "        option=1\n",
    "\n",
    "    if option==0:\n",
    "\n",
    "        X_train=src_list\n",
    "        y_train=tgt_list\n",
    "        z=tokenize_src(X_train)\n",
    "        train_input_ids=z['input_ids']\n",
    "        train_attention_masks=z['attention_mask']\n",
    "        train_labels_ids=tokenize_tgt(y_train)\n",
    "\n",
    "        train_dataset = TensorDataset(train_input_ids,#type(torch.int64)\n",
    "                                    train_attention_masks,\n",
    "                                    train_labels_ids\n",
    "                                    )\n",
    "        train_dataloader = DataLoader(  train_dataset,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        shuffle=True)\n",
    "\n",
    "    if option==1:\n",
    "        train_dataloader=DataLoader(CustomDataset(src_list,tgt_list),\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    shuffle=True,\n",
    "                                    collate_fn=collate)\n",
    "\n",
    "\n",
    "    return train_dataloader\n",
    "\n",
    "#####\n",
    "\n",
    "train_dl=compute_data_dl(src_list,tgt_list,option=traindl_opt)\n",
    "\n",
    "\n",
    "\n",
    "#################################\n",
    "def translate_samples(texts, n=5):\n",
    "\n",
    "    X=tokenize_src(list(texts[:n]))\n",
    "\n",
    "\n",
    "    out = model.generate( X[\"input_ids\"], \n",
    "                         attention_mask=X[\"attention_mask\"],\n",
    "                         num_beams=4,\n",
    "                         max_new_tokens=64, \n",
    "                         early_stopping=True, \n",
    "                         decoder_start_token_id=BOS_ID, \n",
    "                         eos_token_id=EOS_ID, \n",
    "                         pad_token_id=PAD_ID,\n",
    "                         #bad_words_ids=[[PAD_ID]],          # block PAD\n",
    "                         repetition_penalty=1.1,            # mild\n",
    "                         no_repeat_ngram_size=3             # optional hygiene\n",
    "                        )\n",
    "    \n",
    "    return [tok_tgt.decode(o, skip_special_tokens=True) for o in out]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train then test again\n",
    "model.train()\n",
    "opt = AdamW(model.parameters(), lr=LR)\n",
    "steps = 0\n",
    "#print(train_dl.dataset)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    #for X, labels in train_dl:\n",
    "    loss=0\n",
    "    for X,mask, labels in train_dl:\n",
    "        #print(X)\n",
    "        opt.zero_grad()\n",
    "        out=model(input_ids=X,attention_mask=mask,labels=labels)\n",
    "        out.loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        steps += 1\n",
    "        loss+=out.loss\n",
    "    Loss=loss/len(train_dl)\n",
    "#    print(f'Loss: {Loss}')    \n",
    "    print(f\"epoch {epoch+1}/{EPOCHS} done ; loss is {Loss}\")\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "k=5\n",
    "print(\"\\n--- AFTER ---\")\n",
    "preds_after = translate_samples(src_list, n=k)\n",
    "for i in range(k):\n",
    "    print(f\"EN: {src_list[i]}\")\n",
    "    print(f\"FR_gold: {tgt_list[i]}\")\n",
    "    print(f\"FR_pred: {preds_after[i]}\")\n",
    "    print(\"-\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c30d3228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\juano\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:557: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "c:\\Users\\juano\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:577: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/10 done ; loss is 1.0865552425384521\n",
      "epoch 2/10 done ; loss is 0.967190682888031\n",
      "epoch 3/10 done ; loss is 0.7017868757247925\n",
      "epoch 4/10 done ; loss is 0.5001715421676636\n",
      "epoch 5/10 done ; loss is 0.44252726435661316\n",
      "epoch 6/10 done ; loss is 0.3681173026561737\n",
      "epoch 7/10 done ; loss is 0.3237943649291992\n",
      "epoch 8/10 done ; loss is 0.27196842432022095\n",
      "epoch 9/10 done ; loss is 0.23038920760154724\n",
      "epoch 10/10 done ; loss is 0.22900860011577606\n",
      "\n",
      "--- AFTER ---\n",
      "EN: As for me, I found myself obliged, the first time for months, to face alone a long Thursday evening - with the clear feeling that the old carriage had borne away my youth forever.\n",
      "FR_gold: Quant à moi, je me trouvai, pour la première fois depuis de longs mois, seul en face d’une longue soirée de jeudi – avec l’impression que, dans cette vieille voiture, mon adolescence venait de s’en aller pour toujours.\n",
      "FR_pred: Quant à moi, je me trouvai, pour la première fois depuis de longs mois, seul en face d une longue soirée de jeudi avec l impression que, dans cette vieille voiture, mon adolescence venaitait de s en aller pour toujours.\n",
      "-\n",
      "EN: No one asked him who Booby was.\n",
      "FR_gold: Personne ne lui demanda qui était Ganache.\n",
      "FR_pred: Personne ne lui demanda qui était Ganache.\n",
      "-\n",
      "EN: M. Seurel's here .. .'\n",
      "FR_gold: M. Seurel est là…\n",
      "FR_pred: M. Seurel est là\n",
      "-\n",
      "EN: After the ball where everything was charming but feverish and mad, where he had himself so madly chased the tall Pierrot, Meaulnes found that he had dropped into the most peaceful happiness on earth.\n",
      "FR_gold: Après cette fête où tout était charmant, mais fiévreux et fou, où lui-même avait si follement poursuivi le grand pierrot, Meaulnes se trouvait là plongé dans le bonheur le plus calme du monde.\n",
      "FR_pred: Après cette fête où tout était charmant, mais fiévreux et fou, où lui - même avait si follement poursuivi le grand pierrot, Meaulnes se trouvait là plongé dans le bonheur le plus calme du monde.\n",
      "-\n",
      "EN: At half-past eight, just as M. Seurel was giving the signal to enter school, we arrived, quite out of breath, to line up.\n",
      "FR_gold: À huit heures et demie, à l’instant où M. Seurel allait donner le signal d’entrer, nous arrivâmes tout essoufflés pour nous mettre sur les rangs.\n",
      "FR_pred: À huit heures et demie, à l instant où M. Seurel allait donner le signal d entrer, nous arrivâmes tout essoufflés pour nous mettre sur les rangs.\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train then test again\n",
    "model.train()\n",
    "opt = AdamW(model.parameters(), lr=LR)\n",
    "steps = 0\n",
    "#print(train_dl.dataset)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    #for X, labels in train_dl:\n",
    "    loss=0\n",
    "    for X,mask, labels in train_dl:\n",
    "        #print(X)\n",
    "        opt.zero_grad()\n",
    "        #out = model(input_ids=X[\"input_ids\"], attention_mask=X[\"attention_mask\"], labels=labels)\n",
    "        out=model(input_ids=X,attention_mask=mask,labels=labels)\n",
    "        out.loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        steps += 1\n",
    "        loss+=out.loss\n",
    "    Loss=loss/len(train_dl)\n",
    "#    print(f'Loss: {Loss}')    \n",
    "    print(f\"epoch {epoch+1}/{EPOCHS} done ; loss is {Loss}\")\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "k=5\n",
    "print(\"\\n--- AFTER ---\")\n",
    "preds_after = translate_samples(src_list, n=k)\n",
    "for i in range(k):\n",
    "    print(f\"EN: {src_list[i]}\")\n",
    "    print(f\"FR_gold: {tgt_list[i]}\")\n",
    "    print(f\"FR_pred: {preds_after[i]}\")\n",
    "    print(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ced2d91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- AFTER ---\n",
      "EN: As for me, I found myself obliged, the first time for months, to face alone a long Thursday evening - with the clear feeling that the old carriage had borne away my youth forever.\n",
      "FR_gold: Quant à moi, je me trouvai, pour la première fois depuis de longs mois, seul en face d’une longue soirée de jeudi – avec l’impression que, dans cette vieille voiture, mon adolescence venait de s’en aller pour toujours.\n",
      "FR_pred: Quant à moi, je me trouvai, pour la première fois depuis depuis de longs mois, seul en face d une longue soirée de jeudi avec l impression que, dans cette voiture, mon adolescence venait de s en aller pour toujours.\n",
      "-\n",
      "EN: No one asked him who Booby was.\n",
      "FR_gold: Personne ne lui demanda qui était Ganache.\n",
      "FR_pred: Personne ne lui demanda qui était Ganache.\n",
      "-\n",
      "EN: M. Seurel's here .. .'\n",
      "FR_gold: M. Seurel est là…\n",
      "FR_pred: M. Seurel est M..\n",
      "-\n",
      "EN: After the ball where everything was charming but feverish and mad, where he had himself so madly chased the tall Pierrot, Meaulnes found that he had dropped into the most peaceful happiness on earth.\n",
      "FR_gold: Après cette fête où tout était charmant, mais fiévreux et fou, où lui-même avait si follement poursuivi le grand pierrot, Meaulnes se trouvait là plongé dans le bonheur le plus calme du monde.\n",
      "FR_pred: Après cette fête où tout était charmant, mais fiévreux et fou, où lui - même avait si follement poursuivi le grand pierrot, Meaulnes se trouvait là - bas.\n",
      "-\n",
      "EN: At half-past eight, just as M. Seurel was giving the signal to enter school, we arrived, quite out of breath, to line up.\n",
      "FR_gold: À huit heures et demie, à l’instant où M. Seurel allait donner le signal d’entrer, nous arrivâmes tout essoufflés pour nous mettre sur les rangs.\n",
      "FR_pred: À huit heures et demie, à l instant où M. Seurel allait donner le signal d entrer, nous arrivâmes tout essoufflés pour nous mettre sur les rangs.\n",
      "-\n",
      "EN: 'Now then, you !' he said, a little pale, rising and shutting his book, 'get out of it!'\n",
      "FR_gold: – Toi, dit-il, en se redressant et en fermant son livre, un peu pâle, tu vas commencer par sortir d’ici !\n",
      "FR_pred: Toi, dit - il, en se redressant et en fermant son livre, un peu pâle, il va sortir d ici!\n",
      "-\n",
      "EN: I don't know myself,' replied Meaulnes, in the immediate silence.\n",
      "FR_gold: Je ne le sais pas moi-même, répondit Meaulnes, dans le silence déjà grand.\n",
      "FR_pred: Je ne le sais pas moi - même, répondit Meaulnes, dans le silence déjà grand.\n",
      "-\n",
      "EN: Then, as night fell and no more light came from the classroom windows on the throng of boys, Meaulnes used suddenly to get up, and pushing his way through, call out:\n",
      "FR_gold: Puis, à la nuit tombante, lorsque la lueur des carreaux de la classe n’éclairait plus le groupe confus des jeunes gens, Meaulnes se levait soudain et, traversant le cercle pressé :\n",
      "FR_pred: Puis, à la nuit tombante, lorsque la lueur des carreaux de la classe n éclairait plus le groupe confus des jeunes gens, Meaulnes se levait soudain et, traversant le cercle pressé :\n",
      "-\n",
      "EN: 'Not even a rug.\n",
      "FR_gold: Pas même une couverture.\n",
      "FR_pred: Pas même une couverture.\n",
      "-\n",
      "EN: 'Good !\n",
      "FR_gold: – Allons !\n",
      "FR_pred: Allons!\n",
      "-\n",
      "EN: So far as one could see it was a very young man with a long travelling cloak on his shoulders.\n",
      "FR_gold: Autant qu’on pouvait voir, c’était un très jeune homme.\n",
      "FR_pred: Autant qu on pouvait voir, c était un très jeune.\n",
      "-\n",
      "EN: He soon asked to be shown the road to La Motte.\n",
      "FR_gold: Il demanda bientôt qu’on le remît sur la route de La Motte.\n",
      "FR_pred: Il demanda bientôt qu on le remît sur la route de La Motte.\n",
      "-\n",
      "EN: Strange morning !\n",
      "FR_gold: Étrange matinée !\n",
      "FR_pred: Étrange matinée!\n",
      "-\n",
      "EN: He must have asked for the mare to go to the station for Monsieur and Madame Charpentier.\n",
      "FR_gold: Il aura demandé la jument pour aller chercher M. et Mme Charpentier.\n",
      "FR_pred: Il aura demandé la jument pour aller chercher M. et Mme Charpentier.\n",
      "-\n",
      "EN: That's the guide!\n",
      "FR_gold: Voilà le guide !\n",
      "FR_pred: Voilà le guide!\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "def translate_samples(texts, n):\n",
    "\n",
    "    \n",
    "    X = tok_src(list(texts[:n]), \n",
    "            return_tensors=\"pt\",\n",
    "            #padding='max_length',  \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=MAX_SRC_LEN\n",
    "            ).to(device)#\n",
    "    #X=tokenize_src(list(texts[:n]))\n",
    "\n",
    "\n",
    "    out = model.generate( X[\"input_ids\"], \n",
    "                         attention_mask=X[\"attention_mask\"],\n",
    "                         num_beams=4,\n",
    "                         max_new_tokens=64, \n",
    "                         early_stopping=True, \n",
    "                         decoder_start_token_id=BOS_ID, \n",
    "                         eos_token_id=EOS_ID, \n",
    "                         pad_token_id=PAD_ID,\n",
    "                         #bad_words_ids=[[PAD_ID]],          # block PAD\n",
    "                         repetition_penalty=1.1,            # mild\n",
    "                         no_repeat_ngram_size=3             # optional hygiene\n",
    "                        )\n",
    "    \n",
    "    return [tok_tgt.decode(o, skip_special_tokens=True) for o in out]\n",
    "\n",
    "\n",
    "k=15\n",
    "print(\"\\n--- AFTER ---\")\n",
    "preds_after = translate_samples(src_list, n=k)\n",
    "for i in range(k):\n",
    "    print(f\"EN: {src_list[i]}\")\n",
    "    print(f\"FR_gold: {tgt_list[i]}\")\n",
    "    print(f\"FR_pred: {preds_after[i]}\")\n",
    "    print(\"-\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52aaca0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'src_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m n=\u001b[32m4\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m s=\u001b[43msrc_list\u001b[49m[:n]\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#print(s)\u001b[39;00m\n\u001b[32m      4\u001b[39m t=tgt_list[:n]\n",
      "\u001b[31mNameError\u001b[39m: name 'src_list' is not defined"
     ]
    }
   ],
   "source": [
    "n=4\n",
    "s=src_list[:n]\n",
    "#print(s)\n",
    "t=tgt_list[:n]\n",
    "A,a,b=collate((s, t))\n",
    "train_dl=DataLoader(CustomDataset(s, t), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate)\n",
    "A,a,b=zip(*train_dl)\n",
    "\n",
    "\n",
    "#train_input_ids=tokenize_src(X_train)['input_ids']\n",
    "#train_attention_masks = tokenize_src(X_train)['attention_mask']\n",
    "z=tokenize_src(s)\n",
    "train_input_ids=z['input_ids']\n",
    "train_attention_masks=z['attention_mask']\n",
    "train_labels_ids=tokenize_tgt(t)\n",
    "\n",
    "train_dataset = TensorDataset(train_input_ids,#type(torch.int64)\n",
    "                              train_attention_masks,\n",
    "                              train_labels_ids\n",
    "                              )\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)#? not sur if shuffle will be the same in the two loaded\n",
    "\n",
    "\n",
    "mytrain_dl=train_dataloader\n",
    "myA,mya,myb=zip(*mytrain_dl)\n",
    "\n",
    "\n",
    "print(A[0])\n",
    "print('..................')\n",
    "print(myA[0])\n",
    "\n",
    "\n",
    "#print(a[0])\n",
    "print('..................')\n",
    "#print(mya[0][:n])\n",
    "\n",
    "#print(b[0][:n])\n",
    "print('..................')\n",
    "#print(myb[0][:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e4399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mytrain_dl.dataset.tensors[0]\n",
    "#torch.diff(A,myA).sum()\n",
    "(b[0]-myb[0]).sum()\n",
    "\n",
    "\n",
    "q0=model(input_ids=A[0],attention_mask=a[0],labels=b[0])\n",
    "q1=model(input_ids=myA[0],attention_mask=mya[0],labels=myb[0])\n",
    "(q0.logits-q1.logits ).sum()\n",
    "\n",
    "\n",
    "mytrain_dl.dataset.__getitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a0b917",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"Helsinki-NLP/opus_books\", \"en-fr\", split=\"train\")  # ~1M pairs\n",
    "pairs = [(ex[\"translation\"][\"en\"], ex[\"translation\"][\"fr\"]) for ex in ds.select(range(2000))]\n",
    "random.shuffle(pairs)\n",
    "pairs = pairs[:100]  # exactly 100\n",
    "src_list, tgt_list = zip(*pairs)\n",
    "\n",
    "tgt_list=tgt_list[:3]\n",
    "tgt_list=tuple(['Once upon a time','I want to be free'])\n",
    "\n",
    "# target labels: NO BOS; append EOS; mask PAD with -100\n",
    "Y = tok_tgt(list(tgt_list), \n",
    "            padding='max_length',\n",
    "            #padding=True, \n",
    "            truncation=True, \n",
    "            max_length=MAX_TGT_LEN,\n",
    "            add_special_tokens=False,\n",
    "            return_tensors=\"pt\"\n",
    "            )[\"input_ids\"]\n",
    "\n",
    "\n",
    "# append EOS before padding if room\n",
    "Y_fixed = torch.full_like(Y, PAD_ID)\n",
    "for i in range(Y.size(0)):\n",
    "    toks = [t for t in Y[i].tolist() if t != PAD_ID]\n",
    "    if len(toks) < MAX_TGT_LEN:\n",
    "        toks = toks + [EOS_ID]\n",
    "    toks = toks[:MAX_TGT_LEN]\n",
    "    Y_fixed[i, :len(toks)] = torch.tensor(toks, dtype=Y_fixed.dtype)\n",
    "\n",
    "\n",
    "# target labels: NO BOS; append EOS; mask PAD with -100\n",
    "Y = tok_tgt(list(tgt_list), \n",
    "            padding='max_length',\n",
    "            #padding=True, \n",
    "            truncation=True, \n",
    "            max_length=MAX_TGT_LEN,\n",
    "            add_special_tokens=False,\n",
    "            return_tensors=\"pt\"\n",
    "            )[\"input_ids\"]\n",
    "\n",
    "\n",
    "Y_fixed1 = torch.full_like(Y, PAD_ID)\n",
    "for i in range(Y.shape[0]):\n",
    "    y=Y[i,:]\n",
    "    y=y[y!=PAD_ID]\n",
    "    if len(y)<MAX_TGT_LEN:\n",
    "        y=torch.cat((y,torch.tensor([EOS_ID])),dim=0)\n",
    "    Y_fixed1[i,:len(y)]=y\n",
    "\n",
    "\n",
    "#print(Y_fixed[0,:])\n",
    "#print(Y_fixed1[0,:])\n",
    "\n",
    "for i in range(Y.shape[0]):\n",
    "    if torch.diff(Y_fixed[i,:]-Y_fixed1[i,:]).sum() !=0:\n",
    "        print(i)\n",
    "        print (torch.diff(Y_fixed[i,:]-Y_fixed1[i,:]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785ac8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(Pairs(src_list, tgt_list), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate)\n",
    "\n",
    "train_dl.dataset.t\n",
    "#train_dl=train_dataloader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
